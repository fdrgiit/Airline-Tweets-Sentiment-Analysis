---
title: "Analysis of text content of tweets"
author: "Abhinav Singh"
output:
  html_document:
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: readable
  word_document: default
---

# Introduction


```{r include=FALSE}

library(dplyr)
library(readr) # CSV file I/O, e.g. the read_csv function
library(GGally)
library(maps)
library(ggplot2)
library(gridExtra)
library(tm)
library(SnowballC)
library(wordcloud)
library(tidyr)

tweet = read.csv('E:/documents/CourseWork/FODS/Tweets_Final.csv')
data = select(tweet, airline_sentiment, negativereason, airline, tweettext)
head(data)
```

Every tweet begins with a `@airline` tag, which indicates the airline towards which the message is directed. To analyse the content of the tweet, this part is not relevant, so it will be removed from the tweet texts.

```{r, message = FALSE}
# Remove the @airline bit of the text of the tweet
data$tweettext = gsub("^@\\w+ *", "", data$tweettext)
head(data)
```

We can see now, that the tweets do not contain the @airline tag. They are ready to be processed. Tweets classified as negative or positive will be analysed separately.

```{r} 
# divide tweets in 2 dataframes according to positive or negative sentiment
positive = subset(data, airline_sentiment == 'positive')
negative = subset(data, airline_sentiment == 'negative')
dim(positive); dim(negative)
```

We see that from the total amount of tweets (14640), 2363 have been clasified as positive and 9178 as negative.

#Determine word frequency and build cloud of words for each sentiment

```{r} 
# these words appear quite frequently in tweets and in my opinion are not informative,
# so I will remove them"
wordsToRemove = c('get', 'cant', 'can', 'now', 'just', 'will', 'dont', 'ive', 'got', 'much')

# generate a function to analyse corpus text
analyseText = function(text_to_analyse){
    # analyse text and generate matrix of words
    # Returns a dataframe containing 1 tweet per row, one word per column
    # and the number of times the word appears per tweet
    CorpusTranscript = Corpus(VectorSource(text_to_analyse))
    CorpusTranscript = tm_map(CorpusTranscript, content_transformer(tolower), lazy = T)
    CorpusTranscript = tm_map(CorpusTranscript, PlainTextDocument, lazy = T)
    CorpusTranscript = tm_map(CorpusTranscript, removePunctuation)
    CorpusTranscript = tm_map(CorpusTranscript, removeWords, wordsToRemove)
    CorpusTranscript = tm_map(CorpusTranscript, removeWords, stopwords("english"))
    CorpusTranscript = DocumentTermMatrix(CorpusTranscript)
    CorpusTranscript = removeSparseTerms(CorpusTranscript, 0.97) # keeps a matrix 97% sparse
    CorpusTranscript = as.data.frame(as.matrix(CorpusTranscript))
    colnames(CorpusTranscript) = make.names(colnames(CorpusTranscript))
    
    return(CorpusTranscript)
}

words = analyseText(negative$tweettext)
dim(words)
```

The function has extracted 30 words (1 per column) that are repeated with certain frequency accross all negative tweets. Each column specifies how many times that specific word appeared in each tweet (in each row). The sum of the column specifies how many times that specific word was used altogether in all negative tweets.

```{r}
# sum the number of times each word appears in total accross all negative tweets.
freqWords_neg = colSums(words)
freqWords_neg = freqWords_neg[order(freqWords_neg, decreasing = T)]
head(freqWords_neg)
```

We see for example that the word flight appeared 2901 times considering all negative tweets, and the word cancelled appeared 920 times.

```{r}
# analysis of positive tweets
words = analyseText(positive$tweettext)
dim(words)
```

The function has determined 18 words that appear with certain frequency accross positive tweets.

```{r}
freqWords_pos = colSums(words)
freqWords_pos = freqWords_pos[order(freqWords_pos, decreasing = T)]
head(freqWords_pos)
```

As "thank" and "thanks" are conveying the same message, I will sumarise them in one column.

```{r}
freqWords_pos[1] = freqWords_pos[1] + freqWords_pos[2]
freqWords_pos = freqWords_pos[-2]
head(freqWords_pos)
```

We see that the word thanks appears 1061 times accross positive tweets, flight 373 times and so on.

```{r}
#Importing libraries

dat <- select(tweet,airline_sentiment,negativereason,airline,tweettext)

#Cleaning data
dat$tweettext = gsub("^@\\w+ *", "", dat$tweettext)

positive <- subset(dat,airline_sentiment == "positive")
neutral <- subset(dat,airline_sentiment == "neutral")
negative <- subset(dat, airline_sentiment == "negative")

wordsToRemove = c('get', 'cant', 'can', 'now', 'just', 'will', 'dont', 'ive', 'got', 'much')

wc <- function(documents){
  corpusnew <- Corpus(VectorSource(documents))
  corpusnew <- tm_map(corpusnew,content_transformer(tolower))
  corpusnew <- tm_map(corpusnew,removePunctuation)
  corpusnew <- tm_map(corpusnew,removeWords,stopwords("english"))
  corpusnew <- tm_map(corpusnew, removeWords,wordsToRemove)
  corpusnew <- tm_map(corpusnew,stripWhitespace)
  dt <- DocumentTermMatrix(corpusnew)
  dt <- as.data.frame(as.matrix(dt))
  return(dt)
}
```

```{r, message=FALSE, warning=FALSE}
opt <- wc(negative$tweettext)
words <- colnames(opt)
freq_neg <- colSums(opt)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Negative Tweets Cloud", cex=2)
wordcloud(words,freq_neg,min.freq = sort(freq_neg, decreasing = TRUE)[[300]],random.order = FALSE,
          random.color = TRUE, colors = brewer.pal(8, "Dark2"))
```

```{r, message=FALSE, warning=FALSE}
opt <- wc(neutral$tweettext)
words <- colnames(opt)
freq_neu <- colSums(opt)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Neutral Tweets Cloud", cex=2)
wordcloud(words,freq_neu, min.freq = sort(freq_neu, decreasing = TRUE)[[300]],random.order = FALSE,
          random.color = TRUE, colors = brewer.pal(8, "Dark2"))
```

```{r, message=FALSE, warning=FALSE}
opt <- wc(positive$tweettext)
words <- colnames(opt)
freq_pos <- colSums(opt)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Positive Tweets Cloud", cex=2)
wordcloud(words,freq_pos,min.freq = sort(freq_pos, decreasing = TRUE)[[300]],random.order = FALSE,
          random.color = TRUE, colors = brewer.pal(8, "Dark2"))
```

The cloud of words provide a nice visual representation of the word frequency for each type of sentiment. The size of the word correlates with its frequency accross all tweets. We can get an idea of what people are talking about. For example, for negative sentiment, people seem to complain about cancelled or delayed flights, and hours waiting. However, for positive sentiment, people are mostly thankful and they talk about great service/flight.

In the following section, I will analyse the association between words, i.e., words that usually are mentioned together in tweets.

##Association between Words

```{r}
# generate a function to analyse corpus text and return a document term matrix instead of dataframe
# we can perform further analysis on document term matrices
analyseText2 = function(text_to_analyse){
    # analyse text and generate matrix of words
    # Returns a dtm containing 1 tweet per row, one word per column
    # and the number of times the word appears per tweet
    CorpusTranscript = Corpus(VectorSource(text_to_analyse))
    CorpusTranscript = tm_map(CorpusTranscript, content_transformer(tolower), lazy = T)
    CorpusTranscript = tm_map(CorpusTranscript, PlainTextDocument, lazy = T)
    CorpusTranscript = tm_map(CorpusTranscript, removePunctuation)
    CorpusTranscript = tm_map(CorpusTranscript, removeWords, wordsToRemove)
    CorpusTranscript = tm_map(CorpusTranscript, removeWords, stopwords("english"))
    CorpusTranscript = DocumentTermMatrix(CorpusTranscript)
    CorpusTranscript = removeSparseTerms(CorpusTranscript, 0.97) # keeps a matrix 97% sparse
    
    return(CorpusTranscript)
}

words_neg = analyseText2(negative$tweettext)
# find words correlated with the ones mentioned below (correlation at 70%)
findAssocs(words_neg, c("flight", 'customer', 'gate', 'phone'), .07)
```

We see that in negative tweets, the appearance of the word flight correlates with the appearance of the words cancelled, late and delayed, indicating that people are complaining about delayed flights. The word customer is associated with the word service, which is expected, as customer service was a recurrent issue in negative tweets. Interestingly, the word gate is associated with the words waiting and plane, which probably means that people were left waiting at the gate for some time before departure. So from this study, and without having read any tweet, we understand that people are generally complaining about.


```{r}
words_pos = analyseText2(positive$tweettext)
findAssocs(words_pos, c("flight", 'awesome', 'amazing', 'service'), .07)
```

For positive sentiment tweets, we observe that the word flight is associated with great, suggesting that people have had great flight experiences. The word amazing is associated with the word customer, which is in turn associated with the word service, indicating that people experienced an amazing customer service in many opportunities. Similarly, without having actually read any tweet, with this analysis we get an idea of what people are saying about the airlines.

##Clustering Analysis of Words.

```{r}
# hierarchical clustering
d = dist(t(as.matrix(words_neg)), method = 'euclidean')
fit = hclust(d = d, method = 'ward.D')

#fancy plot
op = par(bg = "#DDE3CA")
plot(fit, col = "#487AA1", col.main = "#45ADA8", col.lab = "#7C8071", main = 'Negative Sentiment', xlab = '',
     col.axis = "#F38630", lwd = 3, lty = 3, sub = "", hang = -1, axes = FALSE)
# add axis
axis(side = 2, at = seq(0, 400, 100), col = "#F38630", labels = FALSE, 
     lwd = 2)
# add text in margin
mtext(seq(0, 100, 10), side = 2, at = seq(0, 100, 10), line = 1, 
      col = "#A38630", las = 2)
```

In the dendrogram, words that are linked by short arms are highly associated.

```{r, message=FALSE, warning=FALSE}
plot.new()
plot(fit, hang=-1, main = 'Negative Sentiment', xlab = '')
rect.hclust(fit, k=4, border="red")
```

Although the dendrogram does not seem to be particularly informative, we observe again the association of words like customer and service, and cancelled flight. Words that reflect complains more generally, like waiting, bag (presumably lost), hours, time, hold, cluster altogether.

```{r}
# positive sentiment tweets
d = dist(t(as.matrix(words_pos)), method = 'euclidean')
fit = hclust(d = d, method = 'ward.D')

#fancy plot
op = par(bg = "#DDE3CA")
plot(fit, col = "#487AA1", col.main = "#45ADA8", col.lab = "#7C8071", main = 'Positive Sentiment', xlab = '',
     col.axis = "#F38630", lwd = 3, lty = 3, sub = "", hang = -1, axes = FALSE)
# add axis
axis(side = 2, at = seq(0, 400, 100), col = "#F38630", labels = FALSE, 
     lwd = 2)
# add text in margin
mtext(seq(0, 100, 10), side = 2, at = seq(0, 100, 10), line = 1, 
      col = "#A38630", las = 2)
```

The positive tweet dendrogram is somewhat more informative. We can see the association of customer-service, and best-airline, or love-guys, good-time, which indicate more clearly, what the experience of the airline client was.

```{r, message=FALSE, warning=FALSE}

#Visualizing Airline Sentiment 

T1 <- tweet %>% 
  select(airline_sentiment, airline, negativereason, airline_sentiment_confidence) 

Sentiment <- T1 %>% 
  group_by(airline, airline_sentiment) %>%
  summarise(count=n()) %>% 
  mutate(Percentage=round(count/sum(count)*100,1))

#Overall sentiment towards airlines
ggplot(Sentiment, aes(airline, Percentage)) +
         geom_bar(stat="identity", aes(fill=airline_sentiment)) +
  facet_wrap(~airline_sentiment, nrow = 3) +
  coord_flip() +
  scale_fill_manual(name = "", values = c("#FF715B", "#1E91D6", "#60993E")) +
  theme_minimal() +
      theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())+
  ggtitle("US Airlines Sentiment from Twitter") +
  theme(legend.position="none")
```

The histogram in the first part just depitcts the airline ranking based on the negative, positive and netural feedback shared by the user on twitter. As mentioned by me in my **Exploratory Data Analysis**(earlier submission), the above arrangement related to the negative tweets align with some of the online resources like CNN and Forbes when it comes to worst airline rankings.

```{r}
#Why is there negative sentiment? 

Reason <- T1 %>% 
 select(airline, airline_sentiment, negativereason) %>%
  filter(airline_sentiment=="negative") %>%
  na.omit() %>%
  group_by(airline, negativereason) %>%
  summarise(count=n()) %>%
  mutate(pct=round(count/sum(count)*100,1))

ggplot(Reason, aes(negativereason, pct)) +
  geom_point(size=4, aes(color=negativereason)) +
  facet_wrap(~airline) +
  coord_flip() +
  theme_linedraw() +
  theme(legend.position="none") +
  ylab("% of Negative complaints") +
  xlab("Reason") +
  ggtitle("Reasons for Negative Sentiment by Airline")
```

The second part is just more elaborate part in analyzing the negativity frequency in the negatively polarized tweets. It depicts frequency measures for reasons of the negative sentiment shared by the users per airline. The placement of the dot indicates the count of times the user has mentioned a particular phrase or set of words in his/her tweets related to dissatisfaction for that particular airline.

# Conclusions

I conducted basic text analytics for the tweets. I displayed in word clouds with the frequency of words, the main topics of conversation in tweets with negative and positive sentiment. Then, we found associations between words that allowed us to better understand what the customers were complaining about, or why they enjoyed their flying experience.
